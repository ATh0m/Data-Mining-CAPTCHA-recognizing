{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from src.CAPTCHA_to_signs import DFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "W tym miejscu chcemy podzielić surowe CAPTCHA ze znanymi etykietami na poszczególne znaki z również znanymi etykietami. Powstały zbiór wykorzystamy do nauki modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = os.getcwd()\n",
    "\n",
    "img_dir_path = os.path.join(root_path, 'data', 'raw')\n",
    "invalid_img_dir_path = os.path.join(root_path, 'data', 'invalid')\n",
    "train_img_dir_path = os.path.join(root_path, 'data', 'train')\n",
    "test_img_dir_path = os.path.join(root_path, 'data', 'test') \n",
    "\n",
    "signs_output_dir_path = os.path.join(root_path, 'data', 'signs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obrazki, które można dobrze podzielić przenosimy do folderu **train**, a pozostałe do **invalid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for img_filename in os.listdir(img_dir_path):\n",
    "    img_path = os.path.join(img_dir_path, img_filename)\n",
    "    \n",
    "    label = img_filename.split('.')[0].upper()\n",
    "    signs = DFS(img_path).dfs_all()\n",
    "    \n",
    "    # Przenieś niepoprawne obrazki do folderu invalid\n",
    "    if len(label) != len(signs):\n",
    "        invalid_img_path = os.path.join(invalid_img_dir_path, img_filename)\n",
    "        os.rename(img_path, invalid_img_path)\n",
    "        continue\n",
    "        \n",
    "    # Poprawne przenieś do train\n",
    "    train_img_path = os.path.join(train_img_dir_path, img_filename)\n",
    "    os.rename(img_path, train_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Część obrazków z **train** przenosimy do **test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Przenieś część do test\n",
    "for img_filename in random.sample(os.listdir(train_img_dir_path), 1500):\n",
    "    img_path = os.path.join(train_img_dir_path, img_filename)\n",
    "    \n",
    "    test_img_path = os.path.join(test_img_dir_path, img_filename)\n",
    "    os.rename(img_path, test_img_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy foldery dla wszystkich z mozliwych znaków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sign in '23456789ABCDEFGHJKLMNPQRSTUVWXYZ':\n",
    "    sign_dir_path = os.path.join(signs_output_dir_path, sign)\n",
    "    \n",
    "    if not os.path.exists(sign_dir_path):\n",
    "        os.makedirs(sign_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z obrazków ze zbioru trenującego wydzielamy znaki i zapisujemy je w odpowiednim katalogu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for img_filename in os.listdir(train_img_dir_path):\n",
    "    img_path = os.path.join(train_img_dir_path, img_filename)\n",
    "    \n",
    "    label = img_filename.split('.')[0].upper()\n",
    "    signs = DFS(img_path).dfs_all()\n",
    "        \n",
    "    for sign, target in zip(signs, label):\n",
    "        counter[target] += 1\n",
    "        \n",
    "        sign_path = os.path.join(signs_output_dir_path, target, str(counter[target]))\n",
    "        sign.save(sign_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Implementacja sieci neuronowej o podstawie dla zbioru MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signs_dir_path = os.path.join(root_path, 'data', 'signs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for sign in '23456789ABCDEFGHJKLMNPQRSTUVWXYZ':\n",
    "    sign_dir_path = os.path.join(signs_dir_path, sign)\n",
    "    \n",
    "    for img_filename in os.listdir(sign_dir_path):\n",
    "        img_path = os.path.join(sign_dir_path, img_filename)\n",
    "        \n",
    "        img = image.load_img(img_path, grayscale=True)\n",
    "        img = image.img_to_array(img)\n",
    "        \n",
    "        X.append(img)\n",
    "        y.append(sign)\n",
    "        \n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = len('23456789ABCDEFGHJKLMNPQRSTUVWXYZ')\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 16, 16\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255.\n",
    "X_test /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (21929, 16, 16, 1)\n",
      "21929 train samples\n",
      "5483 test samples\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 317,984\n",
      "Trainable params: 317,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(root_path, 'model')\n",
    "log_path = os.path.join(model_path, 'log')\n",
    "\n",
    "cur_model = 'simple-model-v1'\n",
    "csv_logger = CSVLogger(os.path.join(log_path, cur_model + '.log'))\n",
    "\n",
    "model_file_name= os.path.join(model_path, cur_model + '-{epoch:03d}-{acc:.5f}.h5')\n",
    "checkpoint = ModelCheckpoint(model_file_name, verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21929 samples, validate on 5483 samples\n",
      "Epoch 1/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0028 - acc: 0.9990 - val_loss: 2.5450e-07 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0025 - acc: 0.9992 - val_loss: 2.5087e-07 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0026 - acc: 0.9992 - val_loss: 2.6676e-07 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0026 - acc: 0.9991 - val_loss: 2.3437e-07 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0020 - acc: 0.9994 - val_loss: 1.9748e-07 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0019 - acc: 0.9994 - val_loss: 2.1631e-07 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0018 - acc: 0.9995 - val_loss: 2.1958e-07 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0018 - acc: 0.9995 - val_loss: 1.6189e-07 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0022 - acc: 0.9991 - val_loss: 1.8054e-07 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "21929/21929 [==============================] - 3s - loss: 0.0019 - acc: 0.9992 - val_loss: 3.2731e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe47e32fb90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[csv_logger, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.27305875907e-07\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Przetestowanie modelu na danych testowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "for img_filename in os.listdir(test_img_dir_path):\n",
    "    img_path = os.path.join(test_img_dir_path, img_filename)\n",
    "    \n",
    "    label = img_filename.split('.')[0].upper()\n",
    "    signs = DFS(img_path).dfs_all()\n",
    "    \n",
    "    signs = np.array([s.bitmap for s in signs]).reshape(-1, 16, 16, 1)\n",
    "    predict = model.predict_classes(signs, verbose=0)\n",
    "    \n",
    "    if label == ''.join(encoder.classes_[predict]):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CAPTCHA dataset accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "acc = correct * 100. / len(os.listdir(test_img_dir_path))\n",
    "print(\"Test CAPTCHA dataset accuracy: {}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACSCAYAAABVCTF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni4ikhBq6iEhK/D9Nip1geX7BEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe46c7d8b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_img_filename = random.choice(os.listdir(test_img_dir_path))\n",
    "sample_img_path = os.path.join(test_img_dir_path, sample_img_filename)\n",
    "\n",
    "sample_img = imread(sample_img_path)\n",
    "imshow(sample_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y N 9 N\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABrCAYAAABnlHmpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n83pO9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe46c016890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_signs = DFS(sample_img_path).dfs_all()\n",
    "\n",
    "fig, axes = plt.subplots(ncols=len(sample_signs))\n",
    "for sign, ax in zip(sample_signs, axes):\n",
    "    ax.imshow(sign.bitmap, cmap='gray')\n",
    "    \n",
    "sample_signs_bitmap = np.array([s.bitmap for s in sample_signs]).reshape(-1, 16, 16, 1)\n",
    "predict = model.predict_classes(sample_signs_bitmap, verbose=0)\n",
    "print(' '.join(encoder.classes_[predict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
